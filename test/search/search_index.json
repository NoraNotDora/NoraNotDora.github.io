{"config":{"lang":["zh"],"separator":"[\\s\\-]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"\u2728 Awesome Issue Resolution <p>A Survey of SWE Task: Advances, Frontiers and Future of Issue Resolution</p>          GitHub                 Paper"},{"location":"#awesome-issue-resolution","title":"\ud83e\udd16 Awesome Issue Resolution","text":"<p>A curated list of papers, frameworks, and datasets concerning the use of Large Language Models (LLMs) in tackling Software Engineering Tasks, specifically focusing on Issue Resolution.</p>"},{"location":"#abstract","title":"\ud83d\udcc4 Abstract","text":"<p>Issue Resolution, a complex Software Engineering (SWE) task integral to real-world development, has garnered substantial interest within artificial intelligence. Notably, the establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for Large Language Models (LLMs), which has catalyzed a new research frontier focused on building sophisticated agents. In this paper, we systematically investigate this emerging domain, summarizing the rapidly growing ecosystem of data, methods and analysis through a meticulous taxonomy that offers novel perspectives. Moreover, we delve into the current frontiers and delineate the key challenges and future directions, thereby shedding light on future research. Furthermore, we engage in a discussion about open problems and critical analyses. We hope this paper serves as an introduction for researchers and fosters future research in this domain.</p>"},{"location":"#overview","title":"\ud83d\uddfa\ufe0f Overview","text":"<p>This compilation systematically organizes and categorizes recent advancements in the field of LLM-based Issue Resolution within Software Engineering Tasks (SWE Tasks). The works are structured into three main research areas: Data, Methods, and Analysis, based on the corresponding papers featured in the taxonomy. Through this comprehensive survey, we aim to provide researchers with a clear understanding of the current state-of-the-art, emerging trends, and future research directions in this rapidly evolving domain.</p> <p></p>"},{"location":"#methods","title":"\ud83d\udee0\ufe0f Methods","text":"<p>This section highlights the different techniques and frameworks proposed for resolving SWE tasks, categorized into Training-free Methods and Training-based Methods.</p>"},{"location":"#training-free-methods","title":"\ud83e\uddd1\u200d\ud83d\udcbb Training-free Methods","text":"<p>These approaches generally focus on designing effective Agent Frameworks or Modules (such as tools and memory) to maximize the LLM's reasoning capability without extensive model fine-tuning.</p>"},{"location":"#frameworks","title":"Frameworks","text":"<p>Frameworks typically define the overall structure for how agents interact with the environment and execute tasks.</p> <ul> <li>Single-Agent</li> <li>SWE-Agent: Agent computer interfaces enable reliable evaluation of large language models on repository-level code changes.</li> <li>PatchPilot: A cost-efficient software engineering agent.</li> <li>LCLM: Putting context in context: Simplifying agents with limited context.</li> <li>DGM: Darwin Godel machine: An open-ended developmental machine for software engineering.</li> <li>SE-Agent: SE-agent: Self-evolution trajectory optimization for large language model powered software engineering agents.</li> <li>Live-SWE-agent: Live-SWE-agent: The evolution of self-evolving agents for software engineering.</li> </ul>"},{"location":"#frameworks_1","title":"Frameworks","text":"<p>Frameworks typically define the overall structure for how agents interact with the environment and execute tasks.</p> <ul> <li>Single-Agent</li> <li>SWE-Agent: Agent computer interfaces enable reliable evaluation of large language models on repository-level code changes.</li> <li>PatchPilot: A cost-efficient software engineering agent.</li> <li>LCLM: Putting context in context: Simplifying agents with limited context.</li> <li>DGM: Darwin Godel machine: An open-ended developmental machine for software engineering.</li> <li>SE-Agent: SE-agent: Self-evolution trajectory optimization for large language model powered software engineering agents.</li> <li> <p>Live-SWE-agent: Live-SWE-agent: The evolution of self-evolving agents for software engineering.</p> </li> <li> <p>Multi-Agent</p> </li> <li>MAGIS: An LLM-based multi-agent framework for automatic issue resolution.</li> <li>AutoCodeRover: Autonomous program improvement.</li> <li>CodeR: Issue resolving multi-agent framework for automated software engineering.</li> <li>OpenHands: Open hands: An open platform for AI-native software engineering.</li> <li>OrcaLoca: OrcaLoca: An LLM-agent framework for accurate issue localization and bug fixing in large codebases.</li> <li>DEI: Diversity empowers intelligence: Integrating LLM agent for automated software engineering.</li> <li>MarsCode Agent: An AI-native automated software engineering agent.</li> <li>SWE-Search: SWE-search: Enhancing software agents with web search capabilities.</li> <li>CodeCoR: CodeCoR: LLM-based self-reflective multi-agent framework for software issue resolution.</li> <li>Agent KB: Leveraging cross-domain knowledge graphs for software issue resolution.</li> <li>SWE-Debate: SWE-Debate: A competitive multi-agent debate framework for software engineering tasks.</li> <li>SWE-Exp: SWE-Exp: An experience-driven software issue resolution agent.</li> <li>Trae Agent: An LLM-based agent for real-world software issue resolution.</li> <li> <p>Meta-RAG: Meta-RAG for large codebases using structural code retrieval.</p> </li> <li> <p>Workflow</p> </li> <li>Agentless: Agentless: Demystifying LLM-based software engineering.</li> <li>SynFix: Improving verifiable bug fixing by syntax-guided patch generation.</li> <li>CodeV: CodeV: Towards multimodal understanding and reasoning for software engineering.</li> <li>Seeing is Fixing: Seeing is fixing: Cross-modal reasoning for bug resolution in software engineering.</li> </ul>"},{"location":"#modules","title":"Modules","text":"<p>Core components used to enhance the capabilities of agents within a framework.</p> <ul> <li>Tool</li> <li>(Includes various frameworks and standalone tools, see original mind map for full list of papers)</li> <li>Alibaba LingmaAgent: Improving automated software engineering with an agent-based framework.</li> <li>SpecRover: Code intent extraction and transformation for bug fixing.</li> <li>RepoGraph: RepoGraph: Enhancing AI software engineers with repository-level knowledge graphs.</li> <li>CoSIL: Issue localization in LLM-driven iterative bug fixing.</li> <li>SWERank: Software issue localization using large language models.</li> <li> <p>TestPrune: Old meets new: Evaluating the effectiveness of test pruning for large language models.</p> </li> <li> <p>Memory</p> </li> <li>Infant Agent: Infant agent: Tool-integrated logic-driven planning for complex software tasks.</li> <li>EvoCoder: LLMs as continuous learners: Improving code generation through self-feedback and experience replay.</li> <li>ExpeRepair: ExpeRepair: Dual-memory enhanced LLM-based software repair.</li> <li>commit history-based memory: Improving code localization for repository-level bug fixing using commit history-based memory.</li> <li> <p>ReasoningBank: ReasoningBank: Scaling agent self-evolving in the wild.</p> </li> <li> <p>Inference-time Scaling</p> </li> <li>SWE-Search: SWE-search: Enhancing software agents with web search capabilities.</li> <li>CodeMonkeys: CodeMonkeys: Scaling test-time compute to 2x for a 13% improvement on SWE-Bench.</li> <li>SWE-PRM: Agents astray: Course correcting SWE agents with process reward models.</li> </ul>"},{"location":"#training-based-methods","title":"\ud83e\udde0 Training-based Methods","text":"<p>Focuses on adapting LLMs to SWE tasks through model training and fine-tuning on specific datasets.</p> <ul> <li>SFT-based Methods (Supervised Fine-Tuning)</li> </ul> <ul> <li>Lingma SWE-GPT: An open development process-centric LLM for software engineering. arXiv</li> <li>CodeXEmbed: CodeXEmbed: Code retrieval via representation learning. arXiv</li> <li>SWE-Gym: Training software engineering agents with large language models. arXiv</li> <li>SoRFT: SoRFT: Issue resolving sub-task oriented finetuning for LLMs. arXiv</li> <li>SWE-Dev: SWE-Dev: Building software engineering LLMs. arXiv</li> </ul> <ul> <li>RL-based Methods (Reinforcement Learning)</li> </ul> <ul> <li>SWE-RL: Advancing LLM reasoning in software engineering with reinforcement learning. arXiv</li> <li>SEAlign: SEAlign: Alignment training for software engineering agents. arXiv</li> <li>Satori-SWE: Evolutionary test-time scaling for software engineering agents. arXiv</li> <li>DeepSWE: DeepSWE: Deep learning for software engineering. arXiv</li> <li>DAPO: Training long-context multi-turn software engineering agents via domain-aligned policy optimization. arXiv</li> </ul>"},{"location":"#data","title":"\ud83d\udcc2 Data","text":"<p>This section lists crucial datasets and data construction methodologies used for evaluating and training Issue Resolution models.</p>"},{"location":"#datasets","title":"\ud83d\udcca Datasets","text":""},{"location":"#evaluation-datasets","title":"Evaluation Datasets","text":"<ul> <li>SWE-bench: Language models are good at resolving code changes. arXiv</li> <li>SWE-bench Lite: A light version of SWE-bench. arXiv</li> <li>SWE-bench-java: A GitHub issue resolving benchmark for Java. arXiv</li> <li>Visual SWE-bench: Towards multimodal understanding and reasoning for software engineering. arXiv</li> <li>SWE-Lancer: Frontier LLMs earn money on GitHub. arXiv</li> <li>CodeRAG-bench: A benchmark for evaluating code retrieval-augmented generation. arXiv</li> <li>Multi-SWE-bench: A multilingual benchmark for issue resolution. arXiv</li> <li>SWE-PolyBench: A multi-language benchmark for repository-level software. arXiv</li> <li>SWE-bench Multilingual: Scaling data for software engineering agents. arXiv</li> <li>SWE-fficiency: Optimizing language models for software repair efficiency. arXiv</li> </ul>"},{"location":"#training-datasets","title":"Training Datasets","text":"<ul> <li>SWE-bench-train: Training split of the SWE-bench dataset. arXiv GitHub HuggingFace</li> <li>SWE-bench-extra: Extra training data for SWE-bench. arXiv GitHub</li> <li>Multi-SWE-RL: Training data for multilingual RL. arXiv</li> <li>SWE-Synth: Synthesizing verifiable bugfix datasets. arXiv</li> <li>SWE-Smith: Scaling data for software engineering agents. arXiv</li> <li>SWE-Fixer: Training open-source LLMs for automated bug fixing. arXiv</li> </ul>"},{"location":"#training-free-methods_1","title":"\ud83e\uddd1\u200d\ud83d\udcbb Training-free Methods","text":""},{"location":"#single-agent","title":"Single-Agent","text":"<ul> <li>SWE-Agent: Agent computer interfaces enable reliable evaluation of large language models on repository-level code changes. arXiv GitHub</li> <li>PatchPilot: A cost-efficient software engineering agent.</li> <li>LCLM: Putting context in context: Simplifying agents with limited context.</li> <li>DGM: Darwin Godel machine: An open-ended developmental machine for software engineering.</li> <li>SE-Agent: Self-evolution trajectory optimization for large language model powered software engineering agents.</li> <li>Live-SWE-agent: The evolution of self-evolving agents for software engineering.</li> </ul>"},{"location":"#multi-agent","title":"Multi-Agent","text":"<ul> <li>MAGIS: An LLM-based multi-agent framework for automatic issue resolution.</li> <li>AutoCodeRover: Autonomous program improvement.</li> <li>CodeR: Issue resolving multi-agent framework for automated software engineering.</li> <li>OpenHands: An open platform for AI-native software engineering.</li> <li>OrcaLoca: Accurate issue localization and bug fixing in large codebases.</li> </ul>"},{"location":"#workflow","title":"Workflow","text":"<ul> <li>Agentless, SynFix, CodeV, Seeing is Fixing.</li> </ul>"},{"location":"#training-based-methods_1","title":"\ud83e\udde0 Training-based Methods","text":""},{"location":"#sft-based-methods","title":"SFT-based Methods","text":"<ul> <li>Lingma SWE-GPT \u2022 CodeXEmbed \u2022 SWE-Gym \u2022 SoRFT \u2022 SWE-Dev.</li> </ul>"},{"location":"#rl-based-methods","title":"RL-based Methods","text":"<ul> <li>SWE-RL \u2022 SEAlign \u2022 Satori-SWE \u2022 DeepSWE \u2022 DAPO.</li> </ul>"},{"location":"#evaluation-datasets_1","title":"\ud83d\udcca Evaluation Datasets","text":"<ul> <li>SWE-bench \u2022 SWE-bench Lite \u2022 SWE-bench-java \u2022 Visual SWE-bench</li> <li>SWE-Lancer \u2022 CodeRAG-bench \u2022 Multi-SWE-bench \u2022 SWE-PolyBench</li> <li>SWE-bench Multilingual \u2022 SWE-fficiency</li> </ul>"},{"location":"#data-construction","title":"\ud83c\udfd7\ufe0f Data Construction","text":""},{"location":"#data-collection","title":"Data Collection","text":"<ul> <li>SWE-rebench \u2022 SWE-bench Goes Live! \u2022 RepoForge</li> </ul>"},{"location":"#data-synthesis","title":"Data Synthesis","text":"<ul> <li>Learn-by-interact \u2022 R2E-Gym \u2022 SWE-Flow \u2022 SWE-Mirror</li> </ul> <ul> <li>SWE-bench Verified: Verified LLM-generated fixes for SWE-Bench. arXiv</li> <li>SWE-Bench+: An enhanced coding benchmark. arXiv</li> <li>Patch Correctness: Are LLM-generated bug fixes correct? arXiv</li> <li>SPICE: Automated SWE-Bench labeling and analysis. arXiv</li> </ul> <ul> <li>Context Retrieval: The importance of reasoning for context retrieval. arXiv</li> <li>The Danger of Overthinking: Examining reasoning and action in LLM software agents. arXiv</li> <li>Security Analysis: Analyzing security vulnerabilities in AI-generated fixes. arXiv</li> <li>SeaView: Software engineering agent evaluation. arXiv</li> </ul>"},{"location":"#data-construction_1","title":"\ud83c\udfd7\ufe0f Data Construction","text":"<p>Covers the tools and methods used to build and expand the datasets.</p> <ul> <li>Data Collection</li> </ul> <ul> <li>SWE-rebench: An automated pipeline for software engineering task replication. arXiv</li> <li>SWE-bench Goes Live!: Continuously collecting and evaluating real-world bugs. arXiv</li> <li>RepoForge: Training SOTA fast-thinking LLM agents for complex software tasks. arXiv</li> </ul> <ul> <li>Data Synthesis</li> </ul> <ul> <li>Learn-by-interact: Learning by interacting with the code execution environment. arXiv</li> <li>R2E-Gym: A procedural environment for hybrid reinforcement learning in software engineering. arXiv</li> <li>SWE-Flow: Synthesizing software engineering tasks for training agents. arXiv</li> <li>SWE-Mirror: Scaling issue-resolving datasets with agent collaboration. arXiv</li> </ul>"},{"location":"#analysis","title":"\ud83d\udd0d Analysis","text":"<p>This section includes research works that provide in-depth analysis and discussion of data, methods, and related phenomena in issue resolution.</p> <ul> <li>Data Analysis</li> </ul> <ul> <li>SWE-bench Verified: Verified LLM-generated fixes for SWE-Bench. arXiv GitHub HuggingFace</li> <li>SWE-Bench+: An enhanced coding benchmark. arXiv GitHub</li> <li>Patch Correctness: Are LLM-generated bug fixes correct? arXiv</li> <li>SPICE: Automated SWE-Bench labeling and analysis. arXiv</li> </ul> <ul> <li>Methods Analysis</li> </ul> <ul> <li>Context Retrieval: The importance of reasoning for context retrieval. arXiv GitHub</li> <li>The Danger of Overthinking: Examining reasoning and action in LLM software agents. arXiv</li> <li>Security Analysis: Analyzing security vulnerabilities in AI-generated fixes. arXiv</li> <li>SeaView: Software engineering agent evaluation. arXiv</li> </ul>"},{"location":"about/","title":"\u5173\u4e8e\u672c\u9879\u76ee","text":"<p>\u672c\u9879\u76ee\u6536\u5f55\u5e76\u6574\u7406\u4e86\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u95ee\u9898\u4fee\u590d\uff08Issue Resolution\uff09\u76f8\u5173\u7684\u8bba\u6587\u3001\u65b9\u6cd5\u4e0e\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\u4e0e\u68c0\u7d22\u80fd\u529b\u3002</p> <ul> <li>\u5168\u6587\u641c\u7d22\u7531 MkDocs Material \u63d0\u4f9b\uff0c\u652f\u6301\u9ad8\u4eae\u4e0e\u5efa\u8bae</li> <li>\u4fa7\u680f\u76ee\u5f55\u968f\u6eda\u52a8\u6d6e\u52a8\uff0c\u4fbf\u4e8e\u5feb\u901f\u5b9a\u4f4d</li> <li>\u5185\u5bb9\u6765\u6e90\u4e8e\u4ed3\u5e93\u6839\u76ee\u5f55\u7684 <code>README.md</code></li> </ul>"},{"location":"cite/","title":"Cite","text":"<p>\u5982\u679c\u60a8\u5728\u7814\u7a76\u6216\u7cfb\u7edf\u4e2d\u4f7f\u7528\u672c\u9879\u76ee\u6216\u76f8\u5173\u7efc\u8ff0\uff0c\u8bf7\u5f15\u7528\u4e0b\u65b9\u7684 BibTeX\uff1a</p> <pre><code>@misc{awesome_issue_resolution_survey_2025,\n  title        = {Awesome Issue Resolution: A Survey on LLM-based Software Engineering Issue Resolution},\n  howpublished = {\\url{https://github.com/NoraNotDora/awesome-issue-resolution}},\n  note         = {Version 1.0},\n  year         = {2025}\n}\n</code></pre> <p>\u5982\u5df2\u53d1\u5e03\u81f3 arXiv/\u4f1a\u8bae\uff0c\u8bf7\u5c06\u6761\u76ee\u66ff\u6362\u4e3a\u6b63\u5f0f\u7684\u5f15\u7528\u4fe1\u606f\uff08\u4f5c\u8005\u3001doi/arXiv\u3001\u4f1a\u8bae\u540d\u7b49\uff09\u3002</p>"}]}